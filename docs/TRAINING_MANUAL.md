# ðŸŽ“ MANUAL DE TREINAMENTO - RegressÃ£o Multi-Target

## COMO EXECUTAR O TREINAMENTO COMPLETO

### â±ï¸ TEMPO ESTIMADO

### **Setup Atual (CPU)**
- **Dataset**: 2.1M samples, 61 features
- **Modelo**: Random Forest (200 Ã¡rvores, depth=15)
- **Tempo esperado**: 30-60 minutos (depende do CPU)
- **MemÃ³ria RAM**: ~8-16GB

### **Setup Futuro (GPU - GTX 1060 3GB)**
- **Modelo**: LightGBM com GPU acceleration
- **Tempo esperado**: 3-8 minutos âš¡ (6-10x mais rÃ¡pido)
- **VRAM**: 3GB (suficiente)
- **Status**: SerÃ¡ implementado apÃ³s POC validado

---

## ðŸ“ PASSO A PASSO

### 1ï¸âƒ£ Ativar ambiente virtual

```bash
cd "C:\projetos\Fila de IdÃ©ias\TradeSigns\emDevComManus\tradesigns-poc-live"
.\venv\Scripts\activate
```

### 2ï¸âƒ£ Executar treinamento

```bash
python scripts/train_model_regression.py
```

**O que vai acontecer:**
1. Carrega dataset (2.1M samples) - ~5 segundos
2. Remove features com NaN - ~5 segundos
3. Remove linhas com infinitos - ~5 segundos
4. Split train/test (80/20) - instantÃ¢neo
5. **TREINA MODELO** - **30-60 minutos** â³
6. Avalia no test set - ~2 minutos
7. Calcula feature importance - ~1 minuto
8. Salva modelo - ~5 segundos

---

## ðŸ“Š OUTPUT ESPERADO

### Durante o treinamento:
```
2025-10-28 10:43:41 - INFO - Treinando modelo...
(aguarde... sem output por 30-60 min - ISSO Ã‰ NORMAL!)
```

### ApÃ³s conclusÃ£o:
```
âœ“ Modelo treinado!
   Tempo: 45.3 minutos

MÃ‰TRICAS DE TESTE:
==================
return_5m:
   RMSE: 0.0287%
   MAE:  0.0201%
   RÂ²:   0.2342

return_10m:
   RMSE: 0.0402%
   MAE:  0.0289%
   RÂ²:   0.2567

return_30m:
   RMSE: 0.0698%
   MAE:  0.0512%
   RÂ²:   0.2891

âœ“ Modelo salvo: models/regression_model.joblib
âœ“ Feature importance: models/feature_importance.csv
```

---

## ðŸŽ¯ INTERPRETAÃ‡ÃƒO DOS RESULTADOS

### RÂ² Score (Coeficiente de DeterminaÃ§Ã£o)
- **RÂ² > 0.20**: EXCELENTE para Forex (mercado eficiente)
- **RÂ² > 0.25**: MUITO BOM
- **RÂ² > 0.30**: EXCEPCIONAL

**Por quÃª RÂ² "baixo" Ã© bom aqui?**
- Forex Ã© um mercado eficiente (difÃ­cil de prever)
- RÂ² = 0.25 significa: explica 25% da variÃ¢ncia dos returns
- Isso Ã© **suficiente para gerar alpha** e lucrar

### RMSE/MAE (Erro MÃ©dio)
- **RMSE < 0.05%**: Bom (5 pips de erro)
- **MAE < 0.03%**: Muito bom (3 pips de erro)

### Feature Importance
- Verifica quais features sÃ£o mais importantes
- **TOP 10 esperadas**:
  1. atr_14 (volatilidade)
  2. rsi_14 (momentum)
  3. bb_position (posiÃ§Ã£o nas Bandas)
  4. ema_9, ema_21 (tendÃªncia)
  5. macd_signal (momentum)
  6. stoch_k (oscilador)
  7. price_vs_ema_200 (tendÃªncia longa)
  8. recent_high_low (suporte/resistÃªncia)
  9. hour, day_of_week (sazonalidade)

---

## âš ï¸ TROUBLESHOOTING

### Processo travado (sem output por >60 min)
- Verifique Task Manager: CPU deve estar 100%
- Verifique RAM: nÃ£o deve estar no limite
- **Se CPU baixo (<50%)**: Pode ter travado
  - Pressione Ctrl+C
  - Verifique se tem outro processo consumindo recursos
  - Tente novamente

### Erro: "MemoryError"
- Dataset muito grande para RAM disponÃ­vel
- **SoluÃ§Ã£o 1**: Fechar outros programas
- **SoluÃ§Ã£o 2**: Reduzir dataset temporariamente:
  ```python
  # Em train_model_regression.py, linha ~42
  df = df.sample(n=500000, random_state=42)  # Usar apenas 500k samples
  ```

### Modelo nÃ£o performa bem (RÂ² < 0.10)
- Features podem nÃ£o ser preditivas
- Revisar feature engineering
- Testar perÃ­odos diferentes (regime de mercado)

---

## ðŸ”§ OTIMIZAÃ‡ÃƒO DE PARÃ‚METROS (Opcional)

Se quiser experimentar com hiperparÃ¢metros:

### Mais Ã¡rvores = Melhor (mas mais lento)
```python
trainer = MultiTargetRegressionTrainer(
    n_estimators=300,  # De 200 para 300 (+50% tempo)
)
```

### Ãrvores mais profundas = Mais expressivo
```python
trainer = MultiTargetRegressionTrainer(
    max_depth=20,  # De 15 para 20 (cuidado com overfit)
)
```

### Grid Search (mais cientÃ­fico, MUITO lento)
```bash
# Cria grid_search_regression.py
python scripts/grid_search_regression.py
# Tempo: 6-12 horas!
```

---

## ðŸ“ ARQUIVOS GERADOS

ApÃ³s o treinamento:

```
models/
â”œâ”€â”€ regression_model.joblib          (Modelo treinado - ~500MB)
â”œâ”€â”€ feature_importance.csv           (ImportÃ¢ncia das features)
â””â”€â”€ training_metrics.json            (MÃ©tricas detalhadas)

logs/
â””â”€â”€ train_regression_model.log       (Log completo)
```

---

## âœ… PRÃ“XIMOS PASSOS

ApÃ³s o modelo treinado:

### PASSO 3: Gerar sinais e backtest
```bash
python scripts/generate_signals_regression.py
```

**Tempo**: ~5-10 minutos

**Output esperado**:
- Sinais gerados: 50,000-100,000
- Win Rate: 52-55%
- Sharpe Ratio: 1.5-2.0
- Max Drawdown: <15%

---

## ðŸ’¡ DICAS

1. **Execute durante a noite** - Processo longo, deixe rodando
2. **Monitore a primeira vez** - Garanta que nÃ£o trave
3. **Salve logs** - Ãštil para debug depois
4. **CPU melhor = mais rÃ¡pido** - Considere cloud se necessÃ¡rio
5. **n_jobs=-1** - JÃ¡ configurado para usar todos os cores

---

## ðŸš€ MELHORIAS FUTURAS

### **PRÃ“XIMA ITERAÃ‡ÃƒO (PÃ³s-POC):**

#### ðŸ”¥ **1. LightGBM com GPU (GTX 1060 3GB)** - PRIORIDADE!
**Por quÃª?**
- âš¡ 6-10x mais rÃ¡pido (3-8 min vs 30-60 min)
- ðŸŽ¯ Performance igual ou melhor que Random Forest
- ðŸ’¾ 3GB VRAM suficiente para nosso dataset
- ðŸ”„ Permite retreinos diÃ¡rios/semanais (walk-forward)

**Como implementar:**
```bash
# 1. Instalar LightGBM com suporte GPU
pip install lightgbm --install-option=--gpu

# 2. Instalar dependÃªncias CUDA (se ainda nÃ£o tiver)
# Download: https://developer.nvidia.com/cuda-downloads
# VersÃ£o recomendada: CUDA 11.8 ou 12.x

# 3. Executar novo script
python scripts/train_model_lightgbm_gpu.py
```

**Arquivo serÃ¡ criado:** `scripts/train_model_lightgbm_gpu.py`

---

### **Outras Melhorias (MÃ©dio Prazo):**

2. **XGBoost com GPU** - Alternativa ao LightGBM
3. **Optuna** - Auto-tune de hiperparÃ¢metros
4. **Feature Selection** - Reduz de 61 para 30 features top
5. **Ensemble** - Combina RF + LightGBM + XGBoost

---

## ðŸŽ® **HARDWARE ROADMAP**

| Fase | Hardware | Algoritmo | Tempo Treino | Status |
|------|----------|-----------|--------------|--------|
| **POC** | CPU only | Random Forest | 30-60 min | âœ… Atual |
| **ProduÃ§Ã£o V1** | GTX 1060 3GB | LightGBM GPU | 3-8 min | ðŸ”œ PrÃ³ximo |
| **ProduÃ§Ã£o V2** | GTX 1060 3GB | Ensemble | 10-20 min | ðŸ“‹ Futuro |
| **Scale** | RTX 3060 12GB | Neural Network | 5-15 min | ðŸŒŸ Opcional |

---

**Boa sorte! ðŸŽ¯**

Qualquer dÃºvida, consulte os logs em `logs/train_regression_model.log`
